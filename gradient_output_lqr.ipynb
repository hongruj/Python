{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a129091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.defaults import Main\n",
    "import json\n",
    "from lib.controller import dynamic, decompose\n",
    "from lib.lqr import classical_lqr\n",
    "from lib.simple import Sim\n",
    "from scipy.linalg import solve_lyapunov, solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547fad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params I need \n",
    "main = Main()\n",
    "a = main.a\n",
    "r2 = main.r2_dynamic\n",
    "pmd_slice = main.pmd_slice\n",
    "taus = main.taus\n",
    "tau = main.tau\n",
    "# prms\n",
    "n = main.n\n",
    "n_e = main.n_e\n",
    "\n",
    "with open('setup_prms.json') as f:\n",
    "    prms = json.load(f)\n",
    "c = np.array(prms['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9887e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_z, tau_y = main.taus\n",
    "q = Sim().q_mov(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abda9f7",
   "metadata": {},
   "source": [
    "# I pick out codes from controller.dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069267fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, _ = a.shape\n",
    "n_pmd = len(pmd_slice)\n",
    "n_i = n - n_e\n",
    "tau_z, tau_y = taus\n",
    "const_y = (tau / tau_y) * np.eye(n_e)\n",
    "const_z = (tau / tau_z) * np.eye(n_e)\n",
    "\n",
    "yx = 0.1\n",
    "x = np.random.binomial(1, yx, size=(n_e, n_e)) / (yx * n_e)\n",
    "yx = np.hstack((x, np.zeros((n_e, n_i))))\n",
    "\n",
    "a = np.block([\n",
    "    [a, np.zeros((n, n_e)), np.zeros((n, n_e))],\n",
    "    [(tau / tau_y) * yx, -const_y, np.zeros((n_e, n_e))],\n",
    "    [np.zeros((n_e, n)), const_z, -const_z]\n",
    "])\n",
    "\n",
    "b = np.block([\n",
    "    [np.eye(n)],\n",
    "    [np.zeros((n_e + n_e, n))]\n",
    "])\n",
    "\n",
    "c = np.hstack((np.zeros((n_e, n)), np.zeros((n_e, n_e)), np.eye(n_e)))\n",
    "\n",
    "q = np.block([\n",
    "    [q, np.zeros((n, n_e)), np.zeros((n, n_e))],\n",
    "    [np.zeros((n_e, n)), np.zeros((n_e, n_e)), np.zeros((n_e, n_e))],\n",
    "    [np.zeros((n_e, n)), np.zeros((n_e, n_e)), np.zeros((n_e, n_e))]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5328a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(params, gradients, lr=3, beta1=0.9, beta2=0.999, eps=1e-8, t=0):\n",
    "    \"\"\"\n",
    "    Adam optimizer implementation in NumPy.\n",
    "    Args:\n",
    "        params (list): List of NumPy arrays representing the trainable parameters.\n",
    "        gradients (list): List of NumPy arrays representing the gradients of the trainable parameters.\n",
    "        lr (float): Learning rate. 1e-0\n",
    "        beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "        beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "        eps (float): Small constant to avoid division by zero.\n",
    "        t (int): Current iteration number (starting from 0).\n",
    "    Returns:\n",
    "        params (list): Updated parameters after applying Adam optimizer.\n",
    "    \"\"\"\n",
    "    # Initialize the first and second moment estimates\n",
    "    m = [np.zeros_like(p) for p in params]\n",
    "    v = [np.zeros_like(p) for p in params]\n",
    "    \n",
    "    # Update iteration number\n",
    "    t += 1\n",
    "    \n",
    "    # Compute the first and second moment estimates\n",
    "    for i in range(len(params)):\n",
    "        m[i] = beta1 * m[i] + (1 - beta1) * gradients[i]\n",
    "        v[i] = beta2 * v[i] + (1 - beta2) * gradients[i] ** 2\n",
    "    \n",
    "    # Bias correction for first and second moment estimates\n",
    "    m_hat = [m[i] / (1 - beta1 ** t) for i in range(len(params))]\n",
    "    v_hat = [v[i] / (1 - beta2 ** t) for i in range(len(params))]\n",
    "    \n",
    "    # Update parameters\n",
    "    for i in range(len(params)):\n",
    "        params[i] -= lr * m_hat[i] / (np.sqrt(v_hat[i]) + eps)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b17978",
   "metadata": {},
   "source": [
    "# I pick out codes from lqr.output_lqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb42f334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/38 | train loss: 503.293667\n",
      "iteration 2/38 | train loss: 534.703156\n",
      "iteration 3/38 | train loss: 521.196449\n",
      "iteration 4/38 | train loss: 506.533778\n",
      "iteration 5/38 | train loss: 491.591272\n",
      "iteration 6/38 | train loss: 476.537924\n",
      "iteration 7/38 | train loss: 461.429420\n",
      "iteration 8/38 | train loss: 446.289490\n",
      "iteration 9/38 | train loss: 431.129957\n",
      "iteration 10/38 | train loss: 415.957373\n",
      "iteration 11/38 | train loss: 400.775662\n",
      "iteration 12/38 | train loss: 385.587319\n",
      "iteration 13/38 | train loss: 370.394006\n",
      "iteration 14/38 | train loss: 355.196870\n",
      "iteration 15/38 | train loss: 339.996733\n",
      "iteration 16/38 | train loss: 324.794196\n",
      "iteration 17/38 | train loss: 309.589710\n",
      "iteration 18/38 | train loss: 294.383618\n",
      "iteration 19/38 | train loss: 279.176189\n",
      "iteration 20/38 | train loss: 263.967634\n",
      "iteration 21/38 | train loss: 248.758123\n",
      "iteration 22/38 | train loss: 233.547791\n",
      "iteration 23/38 | train loss: 218.336752\n",
      "iteration 24/38 | train loss: 203.125096\n",
      "iteration 25/38 | train loss: 187.912903\n",
      "iteration 26/38 | train loss: 172.700235\n",
      "iteration 27/38 | train loss: 157.487148\n",
      "iteration 28/38 | train loss: 142.273688\n",
      "iteration 29/38 | train loss: 127.059896\n",
      "iteration 30/38 | train loss: 111.845806\n",
      "iteration 31/38 | train loss: 96.631447\n",
      "iteration 32/38 | train loss: 81.416846\n",
      "iteration 33/38 | train loss: 66.202026\n",
      "iteration 34/38 | train loss: 50.987005\n",
      "iteration 35/38 | train loss: 35.771803\n",
      "iteration 36/38 | train loss: 20.556434\n",
      "iteration 37/38 | train loss: 5.340913\n",
      "iteration 38/38 | train loss: -9.874749\n"
     ]
    }
   ],
   "source": [
    "n, _ = a.shape\n",
    "iden = np.eye(n)\n",
    "clo, raw = (n_pmd, n_e)\n",
    "npara = clo * raw\n",
    "bt = b.T\n",
    "ct = c.T\n",
    "\n",
    "def k_of(z):\n",
    "    return z @ c\n",
    "\n",
    "def s_of(acl):\n",
    "    return solve_lyapunov(acl, -iden)\n",
    "\n",
    "def p_of(acl, k):\n",
    "    return solve_lyapunov(acl.T, -(q + (r2 * k.T @ bt @ b @ k)))\n",
    "\n",
    "def grad_z(k, p, s):\n",
    "    return 2 * bt @ (p + (r2 * b @ k)) @ s @ ct   \n",
    "\n",
    "\n",
    "def f_df(z):\n",
    "    z = np.reshape(z, (clo, raw))\n",
    "    k = k_of(z)\n",
    "    acl = a + (b @ k) \n",
    "    p = p_of(acl, k)  \n",
    "    return np.trace(p)\n",
    "\n",
    "def grad_func(z):\n",
    "    z = np.reshape(z, (clo, raw))\n",
    "    k = k_of(z)\n",
    "    acl = a + (b @ k) \n",
    "    s = s_of(acl)\n",
    "    p = p_of(acl, k)\n",
    "    g_ = grad_z(k, p, s)    \n",
    "    return g_.reshape(npara)\n",
    "\n",
    "\n",
    "z0 = classical_lqr(r2, q, a, b) @ solve(c @ c.T, c).T\n",
    "z0 = z0.reshape(npara)\n",
    "\n",
    "n_iter = 38\n",
    "for i in range(n_iter):                        \n",
    "    l = f_df(z0)\n",
    "    z0 = np.array(adam_optimizer([z0], grad_func(z0))[0])\n",
    "    print(f'iteration {i+1}/{n_iter} | train loss: {l:.6f}')\n",
    "#     if l < 1e-6:\n",
    "#         print(f'train loss: {l:.6f}')\n",
    "#         break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3734484",
   "metadata": {},
   "source": [
    "# The loss function is computed negative â†‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d93217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import rosen, rosen_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1925f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(params, gradients, lr=1e-1, beta1=0.9, beta2=0.999, eps=1e-8, t=0):\n",
    "    \"\"\"\n",
    "    Adam optimizer implementation in NumPy.\n",
    "    Args:\n",
    "        params (list): List of NumPy arrays representing the trainable parameters.\n",
    "        gradients (list): List of NumPy arrays representing the gradients of the trainable parameters.\n",
    "        lr (float): Learning rate. 1e-0\n",
    "        beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "        beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "        eps (float): Small constant to avoid division by zero.\n",
    "        t (int): Current iteration number (starting from 0).\n",
    "    Returns:\n",
    "        params (list): Updated parameters after applying Adam optimizer.\n",
    "    \"\"\"\n",
    "    # Initialize the first and second moment estimates\n",
    "    m = [np.zeros_like(p) for p in params]\n",
    "    v = [np.zeros_like(p) for p in params]\n",
    "    \n",
    "    # Update iteration number\n",
    "    t += 1\n",
    "    \n",
    "    # Compute the first and second moment estimates\n",
    "    for i in range(len(params)):\n",
    "        m[i] = beta1 * m[i] + (1 - beta1) * gradients[i]\n",
    "        v[i] = beta2 * v[i] + (1 - beta2) * gradients[i] ** 2\n",
    "    \n",
    "    # Bias correction for first and second moment estimates\n",
    "    m_hat = [m[i] / (1 - beta1 ** t) for i in range(len(params))]\n",
    "    v_hat = [v[i] / (1 - beta2 ** t) for i in range(len(params))]\n",
    "    \n",
    "    # Update parameters\n",
    "    for i in range(len(params)):\n",
    "        params[i] -= lr * m_hat[i] / (np.sqrt(v_hat[i]) + eps)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b550bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/100 | train loss: 199.000000\n",
      "iteration 2/100 | train loss: 322.379999\n",
      "iteration 3/100 | train loss: 636.799998\n",
      "iteration 4/100 | train loss: 975.099998\n",
      "iteration 5/100 | train loss: 1217.879999\n",
      "iteration 6/100 | train loss: 1293.500000\n",
      "iteration 7/100 | train loss: 1178.080002\n",
      "iteration 8/100 | train loss: 895.500003\n",
      "iteration 9/100 | train loss: 517.400003\n",
      "iteration 10/100 | train loss: 163.180003\n",
      "iteration 11/100 | train loss: 0.000000\n",
      "train loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "z0 = np.zeros(200)\n",
    "n_iter = 100\n",
    "for i in range(n_iter):                        \n",
    "    l = rosen(z0)\n",
    "    z0 = np.array(adam_optimizer([z0], rosen_der(z0))[0])\n",
    "    print(f'iteration {i+1}/{n_iter} | train loss: {l:.6f}')\n",
    "    if l < 1e-6:\n",
    "        print(f'train loss: {l:.6f}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c3caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

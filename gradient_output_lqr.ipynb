{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a129091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.defaults import Main\n",
    "import json\n",
    "# from lib.controller import dynamic, decompose\n",
    "from lib.lqr import classical_lqr\n",
    "from lib.simple import Sim\n",
    "from scipy.linalg import solve_lyapunov, solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547fad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params I need \n",
    "main = Main()\n",
    "a = main.a\n",
    "r2 = main.r2_dynamic\n",
    "pmd_slice = main.pmd_slice\n",
    "taus = main.taus\n",
    "tau = main.tau\n",
    "# prms\n",
    "n = main.n\n",
    "n_e = main.n_e\n",
    "\n",
    "with open('setup_prms.json') as f:\n",
    "    prms = json.load(f)\n",
    "c = np.array(prms['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9887e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_z, tau_y = main.taus\n",
    "q = Sim().q_mov(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abda9f7",
   "metadata": {},
   "source": [
    "# I pick out codes from controller.dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069267fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, _ = a.shape\n",
    "n_pmd = len(pmd_slice)\n",
    "n_i = n - n_e\n",
    "tau_z, tau_y = taus\n",
    "const_y = (tau / tau_y) * np.eye(n_e)\n",
    "const_z = (tau / tau_z) * np.eye(n_e)\n",
    "\n",
    "yx = 0.1\n",
    "x = np.random.binomial(1, yx, size=(n_e, n_e)) / (yx * n_e)\n",
    "yx = np.hstack((x, np.zeros((n_e, n_i))))\n",
    "\n",
    "a = np.block([\n",
    "    [a, np.zeros((n, n_e)), np.zeros((n, n_e))],\n",
    "    [(tau / tau_y) * yx, -const_y, np.zeros((n_e, n_e))],\n",
    "    [np.zeros((n_e, n)), const_z, -const_z]\n",
    "])\n",
    "\n",
    "b = np.block([\n",
    "    [np.eye(n)],\n",
    "    [np.zeros((n_e + n_e, n))]\n",
    "])\n",
    "\n",
    "c = np.hstack((np.zeros((n_e, n)), np.zeros((n_e, n_e)), np.eye(n_e)))\n",
    "\n",
    "q = np.block([\n",
    "    [q, np.zeros((n, n_e)), np.zeros((n, n_e))],\n",
    "    [np.zeros((n_e, n)), np.zeros((n_e, n_e)), np.zeros((n_e, n_e))],\n",
    "    [np.zeros((n_e, n)), np.zeros((n_e, n_e)), np.zeros((n_e, n_e))]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a265470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=5, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0, lr_schedule=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "        self.lr_schedule = lr_schedule\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Compute the first and second moment estimates\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * gradients[i]**2\n",
    "        \n",
    "        # Bias correction for first and second moment estimates\n",
    "        m_hat = [self.m[i] / (1 - self.beta1**self.t) for i in range(len(params))]\n",
    "        v_hat = [self.v[i] / (1 - self.beta2**self.t) for i in range(len(params))]\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        if self.lr_schedule is not None:\n",
    "            lr = self.learning_rate * self.lr_schedule(self.t)\n",
    "        else:\n",
    "            lr = self.learning_rate\n",
    "        \n",
    "        # Weight decay\n",
    "        gradients = [gradients[i] + self.weight_decay * params[i] for i in range(len(params))]\n",
    "        \n",
    "        # Update parameters\n",
    "        params = [params[i] - lr * m_hat[i] / (np.sqrt(v_hat[i]) + self.epsilon) for i in range(len(params))]\n",
    "        \n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b17978",
   "metadata": {},
   "source": [
    "# I pick out codes from lqr.output_lqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c3fc4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/23 | train loss: 503.293667\n",
      "iteration 2/23 | train loss: 524.149179\n",
      "iteration 3/23 | train loss: 499.975336\n",
      "iteration 4/23 | train loss: 475.156968\n",
      "iteration 5/23 | train loss: 450.179861\n",
      "iteration 6/23 | train loss: 425.139578\n",
      "iteration 7/23 | train loss: 400.067788\n",
      "iteration 8/23 | train loss: 374.978023\n",
      "iteration 9/23 | train loss: 349.877036\n",
      "iteration 10/23 | train loss: 324.768573\n",
      "iteration 11/23 | train loss: 299.654880\n",
      "iteration 12/23 | train loss: 274.537385\n",
      "iteration 13/23 | train loss: 249.417040\n",
      "iteration 14/23 | train loss: 224.294503\n",
      "iteration 15/23 | train loss: 199.170244\n",
      "iteration 16/23 | train loss: 174.044609\n",
      "iteration 17/23 | train loss: 148.917854\n",
      "iteration 18/23 | train loss: 123.790179\n",
      "iteration 19/23 | train loss: 98.661736\n",
      "iteration 20/23 | train loss: 73.532647\n",
      "iteration 21/23 | train loss: 48.403009\n",
      "iteration 22/23 | train loss: 23.272900\n",
      "iteration 23/23 | train loss: -1.857615\n"
     ]
    }
   ],
   "source": [
    "n, _ = a.shape\n",
    "iden = np.eye(n)\n",
    "clo, raw = (n_pmd, n_e)\n",
    "npara = clo * raw\n",
    "bt = b.T\n",
    "ct = c.T\n",
    "\n",
    "def k_of(z):\n",
    "    return z @ c\n",
    "\n",
    "def s_of(acl):\n",
    "    return solve_lyapunov(acl, -iden)\n",
    "\n",
    "def p_of(acl, k):\n",
    "    return solve_lyapunov(acl.T, -(q + (r2 * k.T @ bt @ b @ k)))\n",
    "\n",
    "def grad_z(k, p, s):\n",
    "    return 2 * bt @ (p + (r2 * b @ k)) @ s @ ct   \n",
    "\n",
    "\n",
    "def f_df(z):\n",
    "    z = np.reshape(z, (clo, raw))\n",
    "    k = k_of(z)\n",
    "    acl = a + (b @ k) \n",
    "    p = p_of(acl, k)  \n",
    "    return np.trace(p)\n",
    "\n",
    "def grad_func(z):\n",
    "    z = np.reshape(z, (clo, raw))\n",
    "    k = k_of(z)\n",
    "    acl = a + (b @ k) \n",
    "    s = s_of(acl)\n",
    "    p = p_of(acl, k)\n",
    "    g_ = grad_z(k, p, s)    \n",
    "    return g_.reshape(npara)\n",
    "\n",
    "\n",
    "z0 = classical_lqr(r2, q, a, b) @ solve(c @ c.T, c).T\n",
    "z0 = z0.reshape(npara)\n",
    "\n",
    "n_iter = 23\n",
    "for i in range(n_iter):                        \n",
    "    l = f_df(z0)\n",
    "    z0 = np.array(Adam().update([z0], grad_func(z0))[0])\n",
    "    print(f'iteration {i+1}/{n_iter} | train loss: {l:.6f}')\n",
    "#     if l < 1e-6:\n",
    "#         print(f'train loss: {l:.6f}')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3734484",
   "metadata": {},
   "source": [
    "# The loss function is computed negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b622fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 23/23 | train loss: -1.857615\n"
     ]
    }
   ],
   "source": [
    "print(f'iteration {i+1}/{n_iter} | train loss: {l:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ca5e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.16292319  1.27170805 -0.65291126 ...  0.86361112  0.86361112\n",
      "   0.86361112]\n",
      " [ 1.27170805  2.2762881   0.50678537 ...  0.59251077  0.59251077\n",
      "   0.59251077]\n",
      " [-0.65291126  0.50678537  1.80502186 ...  0.77379659  0.77379659\n",
      "   0.77379659]\n",
      " ...\n",
      " [ 0.86361112  0.59251077  0.77379659 ... -2.90612802 -2.90612802\n",
      "  -2.90612802]\n",
      " [ 0.86361112  0.59251077  0.77379659 ... -2.90612802 -2.90612802\n",
      "  -2.90612802]\n",
      " [ 0.86361112  0.59251077  0.77379659 ... -2.90612802 -2.90612802\n",
      "  -2.90612802]]\n"
     ]
    }
   ],
   "source": [
    "z0 = np.reshape(z0, (clo, raw))\n",
    "k = k_of(z0)\n",
    "acl = a + (b @ k) \n",
    "print(p_of(acl, k)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb79fd4",
   "metadata": {},
   "source": [
    "# adam_optimizer is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d93217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import rosen, rosen_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de237a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=1e-1, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0, lr_schedule=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "        self.lr_schedule = lr_schedule\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Compute the first and second moment estimates\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * gradients[i]**2\n",
    "        \n",
    "        # Bias correction for first and second moment estimates\n",
    "        m_hat = [self.m[i] / (1 - self.beta1**self.t) for i in range(len(params))]\n",
    "        v_hat = [self.v[i] / (1 - self.beta2**self.t) for i in range(len(params))]\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        if self.lr_schedule is not None:\n",
    "            lr = self.learning_rate * self.lr_schedule(self.t)\n",
    "        else:\n",
    "            lr = self.learning_rate\n",
    "        \n",
    "        # Weight decay\n",
    "        gradients = [gradients[i] + self.weight_decay * params[i] for i in range(len(params))]\n",
    "        \n",
    "        # Update parameters\n",
    "        params = [params[i] - lr * m_hat[i] / (np.sqrt(v_hat[i]) + self.epsilon) for i in range(len(params))]\n",
    "        \n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b550bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/100 | train loss: 199.000000\n",
      "iteration 2/100 | train loss: 322.379999\n",
      "iteration 3/100 | train loss: 636.799998\n",
      "iteration 4/100 | train loss: 975.099998\n",
      "iteration 5/100 | train loss: 1217.879999\n",
      "iteration 6/100 | train loss: 1293.500000\n",
      "iteration 7/100 | train loss: 1178.080002\n",
      "iteration 8/100 | train loss: 895.500003\n",
      "iteration 9/100 | train loss: 517.400003\n",
      "iteration 10/100 | train loss: 163.180003\n",
      "iteration 11/100 | train loss: 0.000000\n",
      "train loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "z0 = np.zeros(200)\n",
    "n_iter = 100\n",
    "for i in range(n_iter):                        \n",
    "    l = rosen(z0)\n",
    "    z0 = np.array(Adam().update([z0], rosen_der(z0))[0])\n",
    "    print(f'iteration {i+1}/{n_iter} | train loss: {l:.6f}')\n",
    "    if l < 1e-6:\n",
    "        print(f'train loss: {l:.6f}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c3caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
